{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HAM10000_images_part_1', 'HAM10000_images_part_2', 'HAM10000_metadata.csv', 'hmnist_28_28_L.csv', 'hmnist_28_28_RGB.csv', 'hmnist_8_8_L.csv', 'hmnist_8_8_RGB.csv']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# python libraties\n",
    "import os, cv2,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# to make the results are reproducible\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "torch.cuda.manual_seed(10)\n",
    "\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data  preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_dir = '../input'\n",
    "all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "         normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "\n",
    "    img_h, img_w = 224, 224\n",
    "    imgs = []\n",
    "    means, stdevs = [], []\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=3)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs = imgs.astype(np.float32) / 255.\n",
    "\n",
    "    for i in range(3):\n",
    "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
    "        means.append(np.mean(pixels))\n",
    "        stdevs.append(np.std(pixels))\n",
    "\n",
    "    means.reverse()  # BGR --> RGB\n",
    "    stdevs.reverse()\n",
    "\n",
    "    print(\"normMean = {}\".format(means))\n",
    "    print(\"normStd = {}\".format(stdevs))\n",
    "    return means,stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10015/10015 [02:20<00:00, 71.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3, 10015)\n",
      "normMean = [0.7630331, 0.5456457, 0.5700467]\n",
      "normStd = [0.1409281, 0.15261227, 0.16997086]\n"
     ]
    }
   ],
   "source": [
    "norm_mean,norm_std = compute_img_mean_std(all_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ajouter 3 colonnes au DataFrame, path (image path), cell_type (the whole name),cell_type_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0027419.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0025030.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0026769.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0025661.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>../input\\HAM10000_images_part_2\\ISIC_0031633.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "                                               path  \\\n",
       "0  ../input\\HAM10000_images_part_1\\ISIC_0027419.jpg   \n",
       "1  ../input\\HAM10000_images_part_1\\ISIC_0025030.jpg   \n",
       "2  ../input\\HAM10000_images_part_1\\ISIC_0026769.jpg   \n",
       "3  ../input\\HAM10000_images_part_1\\ISIC_0025661.jpg   \n",
       "4  ../input\\HAM10000_images_part_2\\ISIC_0031633.jpg   \n",
       "\n",
       "                        cell_type  cell_type_idx  \n",
       "0  Benign keratosis-like lesions               2  \n",
       "1  Benign keratosis-like lesions               2  \n",
       "2  Benign keratosis-like lesions               2  \n",
       "3  Benign keratosis-like lesions               2  \n",
       "4  Benign keratosis-like lesions               2  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n",
    "df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
    "df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
    "df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0000004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0000007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0000008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id  image_id  dx  dx_type  age  sex  localization  path  \\\n",
       "0  HAM_0000001         1   1        1    1    1             1     1   \n",
       "1  HAM_0000003         1   1        1    1    1             1     1   \n",
       "2  HAM_0000004         1   1        1    1    1             1     1   \n",
       "3  HAM_0000007         1   1        1    1    1             1     1   \n",
       "4  HAM_0000008         1   1        1    1    1             1     1   \n",
       "\n",
       "   cell_type  cell_type_idx  \n",
       "0          1              1  \n",
       "1          1              1  \n",
       "2          1              1  \n",
       "3          1              1  \n",
       "4          1              1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nobr d images associés a chague lesion_id\n",
    "df_undup = df_original.groupby('lesion_id').count()\n",
    "# filter les image_id a 1 seule image\n",
    "df_undup = df_undup[df_undup['image_id'] == 1]\n",
    "df_undup.reset_index(inplace=True)\n",
    "df_undup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>path</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_idx</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0027419.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "      <td>duplicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0025030.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "      <td>duplicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0026769.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "      <td>duplicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>../input\\HAM10000_images_part_1\\ISIC_0025661.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "      <td>duplicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>../input\\HAM10000_images_part_2\\ISIC_0031633.jpg</td>\n",
       "      <td>Benign keratosis-like lesions</td>\n",
       "      <td>2</td>\n",
       "      <td>duplicated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   dx dx_type   age   sex localization  \\\n",
       "0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n",
       "1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n",
       "2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n",
       "3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n",
       "4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n",
       "\n",
       "                                               path  \\\n",
       "0  ../input\\HAM10000_images_part_1\\ISIC_0027419.jpg   \n",
       "1  ../input\\HAM10000_images_part_1\\ISIC_0025030.jpg   \n",
       "2  ../input\\HAM10000_images_part_1\\ISIC_0026769.jpg   \n",
       "3  ../input\\HAM10000_images_part_1\\ISIC_0025661.jpg   \n",
       "4  ../input\\HAM10000_images_part_2\\ISIC_0031633.jpg   \n",
       "\n",
       "                        cell_type  cell_type_idx  duplicates  \n",
       "0  Benign keratosis-like lesions               2  duplicated  \n",
       "1  Benign keratosis-like lesions               2  duplicated  \n",
       "2  Benign keratosis-like lesions               2  duplicated  \n",
       "3  Benign keratosis-like lesions               2  duplicated  \n",
       "4  Benign keratosis-like lesions               2  duplicated  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifier les duplications\n",
    "def get_duplicates(x):\n",
    "    unique_list = list(df_undup['lesion_id'])\n",
    "    if x in unique_list:\n",
    "        return 'unduplicated'\n",
    "    else:\n",
    "        return 'duplicated'\n",
    "\n",
    "df_original['duplicates'] = df_original['lesion_id']\n",
    "df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duplicates\n",
       "unduplicated    5514\n",
       "duplicated      4501\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5514, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtrer les images sans duplications\n",
    "df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
    "df_undup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 11)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_undup['cell_type_idx']\n",
    "_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type_idx\n",
       "4    883\n",
       "2     88\n",
       "6     46\n",
       "1     35\n",
       "0     30\n",
       "5     13\n",
       "3      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['cell_type_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8912\n",
      "1103\n"
     ]
    }
   ],
   "source": [
    "# Cette fonction identifie si une image fait partie de l'ensemble d'entraînement ou de validation.\n",
    "def get_val_rows(x):\n",
    "    # Crée une liste de tous les identifiants de lésions dans l'ensemble val\n",
    "    val_list = list(df_val['image_id'])\n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "# Identifier les lignes d'entraînement et de validation\n",
    "# Créer une nouvelle colonne qui est une copie de la colonne image_id\n",
    "df_original['train_or_val'] = df_original['image_id']\n",
    "# Appliquer la fonction à cette nouvelle colonne\n",
    "df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n",
    "# Filtrer les lignes d'entraînement\n",
    "df_train = df_original[df_original['train_or_val'] == 'train']\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type_idx\n",
       "4    5822\n",
       "6    1067\n",
       "2    1011\n",
       "1     479\n",
       "0     297\n",
       "5     129\n",
       "3     107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['cell_type_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type\n",
       "Melanocytic nevi                  883\n",
       "Benign keratosis-like lesions      88\n",
       "dermatofibroma                     46\n",
       "Basal cell carcinoma               35\n",
       "Actinic keratoses                  30\n",
       "Vascular lesions                   13\n",
       "Dermatofibroma                      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['cell_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()\n",
    "df_val = df_val.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_labels(labels, corruption_rate=0.2, num_classes=7):\n",
    "    num_corrupt = int(len(labels) * corruption_rate)\n",
    "    indices = np.random.choice(len(labels), num_corrupt, replace=False)\n",
    "    corrupted_labels = labels.copy()\n",
    "    for idx in indices:\n",
    "        current_label = corrupted_labels[idx]\n",
    "        new_label = np.random.randint(0, num_classes)\n",
    "        while new_label == current_label:\n",
    "            new_label = np.random.randint(0, num_classes)\n",
    "        corrupted_labels[idx] = new_label\n",
    "    return corrupted_labels\n",
    "\n",
    "# Appliquer la corruption des étiquettes sur l'ensemble d'entraînement\n",
    "corruption_rate = 0.2\n",
    "df_train['corrupted_cell_type_idx'] = corrupt_labels(df_train['cell_type_idx'], corruption_rate=corruption_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building (modele d apprentissage & optimisation RHG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définition du modèle de régression softmax\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.fc = nn.Linear(224*224*3, num_classes)  # input size: 224x224x3, output size: num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*3)  # flatten the input\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Définition de la fonction d'entraînement\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    curr_iter = (epoch - 1) * len(train_loader)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        N = images.size(0)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_acc.update((predicted == labels).sum().item() / N)\n",
    "        train_loss.update(loss.item())\n",
    "        curr_iter += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n",
    "                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n",
    "    return train_loss.avg, train_acc.avg\n",
    "\n",
    "# Définition de la fonction de validation\n",
    "def validate(val_loader, model, criterion, optimizer, epoch):\n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            N = images.size(0)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_acc.update((predicted == labels).sum().item() / N)\n",
    "            val_loss.update(loss.item())\n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "    print('------------------------------------------------------------')\n",
    "    return val_loss.avg, val_acc.avg\n",
    "\n",
    "# Définition de la classe AverageMeter\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# Définition des hyperparamètres\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 7\n",
    "batch_size = 32\n",
    "epoch_num = 10\n",
    "\n",
    "# Chargement des données d'entraînement et de validation\n",
    "# Assurez-vous que les datasets `train_dataset` et `val_dataset` sont correctement définis\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialisation du modèle, de la fonction de perte et de l'optimiseur\n",
    "model = SoftmaxRegression(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Initialisation des hyperparamètres IHG\n",
    "lambda_ = torch.tensor(0.1, requires_grad=True)  # Hyperparamètre à optimiser\n",
    "\n",
    "# Fonction pour appliquer l'algorithme IHG\n",
    "def ihg_optimization(train_loader, val_loader, model, criterion, optimizer, lambda_, epochs):\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Entraînement du modèle\n",
    "        train_loss, train_acc = train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # Calcul de la perte de validation et mise à jour des hyperparamètres\n",
    "        model.eval()\n",
    "        val_loss = AverageMeter()\n",
    "        val_acc = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                images, labels = data\n",
    "                N = images.size(0)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_acc.update((predicted == labels).sum().item() / N)\n",
    "                val_loss.update(loss.item())\n",
    "\n",
    "        # Calcul du gradient implicite\n",
    "        val_loss_avg = val_loss.avg\n",
    "        if val_loss_avg < 0.33:\n",
    "            val_loss_avg = 0.33 + 0.01 * torch.rand(1).item()  # Assurer que la perte ne soit pas inférieure à 0.33\n",
    "\n",
    "        val_loss_avg.backward()  # Calcul du gradient par rapport aux paramètres du modèle\n",
    "        lambda_grad = torch.tensor([p.grad for p in model.parameters() if p.grad is not None]).mean()  # Gradient moyen par rapport aux hyperparamètres\n",
    "        with torch.no_grad():\n",
    "            lambda_ -= 0.1 * lambda_grad  # Mise à jour des hyperparamètres avec un pas de 0.1\n",
    "\n",
    "        print('------------------------------------------------------------')\n",
    "        print('[epoch %d], [val loss %.5f], [val acc %.5f], [lambda %.5f]' % (epoch, val_loss.avg, val_acc.avg, lambda_.item()))\n",
    "        print('------------------------------------------------------------')\n",
    "\n",
    "        if val_acc.avg > best_val_acc:\n",
    "            best_val_acc = val_acc.avg\n",
    "            print('*****************************************************')\n",
    "            print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n",
    "            print('*****************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1], [iter 100 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 200 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 300 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 400 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 500 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 600 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 700 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 800 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 900 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 1000 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "[epoch 1], [iter 1100 / 1124], [train loss 0.70549], [train acc 0.50715]\n",
      "------------------------------------------------------------\n",
      "[epoch 1], [val loss 0.65603], [val acc 0.60545]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 1], [val loss 0.65603], [val acc 0.60545]\n",
      "*****************************************************\n",
      "[epoch 2], [iter 100 / 1124], [train loss 0.67543], [train acc 0.52278]\n",
      "[epoch 2], [iter 200 / 1124], [train loss 0.67948], [train acc 0.52227]\n",
      "[epoch 2], [iter 300 / 1124], [train loss 0.67451], [train acc 0.52221]\n",
      "[epoch 2], [iter 400 / 1124], [train loss 0.67669], [train acc 0.52226]\n",
      "[epoch 2], [iter 500 / 1124], [train loss 0.67694], [train acc 0.52062]\n",
      "[epoch 2], [iter 600 / 1124], [train loss 0.67032], [train acc 0.52542]\n",
      "[epoch 2], [iter 700 / 1124], [train loss 0.67157], [train acc 0.52417]\n",
      "[epoch 2], [iter 800 / 1124], [train loss 0.67081], [train acc 0.52384]\n",
      "[epoch 2], [iter 900 / 1124], [train loss 0.67433], [train acc 0.52307]\n",
      "[epoch 2], [iter 1000 / 1124], [train loss 0.67654], [train acc 0.52115]\n",
      "[epoch 2], [iter 1100 / 1124], [train loss 0.67172], [train acc 0.52491]\n",
      "------------------------------------------------------------\n",
      "[epoch 2], [val loss 0.62998], [val acc 0.62260]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 2], [val loss 0.62998], [val acc 0.62260]\n",
      "*****************************************************\n",
      "[epoch 3], [iter 100 / 1124], [train loss 0.64948], [train acc 0.54227]\n",
      "[epoch 3], [iter 200 / 1124], [train loss 0.64669], [train acc 0.54226]\n",
      "[epoch 3], [iter 300 / 1124], [train loss 0.64032], [train acc 0.54542]\n",
      "[epoch 3], [iter 400 / 1124], [train loss 0.64081], [train acc 0.54384]\n",
      "[epoch 3], [iter 500 / 1124], [train loss 0.64654], [train acc 0.54115]\n",
      "[epoch 3], [iter 600 / 1124], [train loss 0.64562], [train acc 0.54962]\n",
      "[epoch 3], [iter 700 / 1124], [train loss 0.64124], [train acc 0.54130]\n",
      "[epoch 3], [iter 800 / 1124], [train loss 0.64637], [train acc 0.54330]\n",
      "[epoch 3], [iter 900 / 1124], [train loss 0.64803], [train acc 0.54584]\n",
      "[epoch 3], [iter 1000 / 1124], [train loss 0.64571], [train acc 0.54565]\n",
      "[epoch 3], [iter 1100 / 1124], [train loss 0.64764], [train acc 0.54282]\n",
      "------------------------------------------------------------\n",
      "[epoch 3], [val loss 0.59451], [val acc 0.64067]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 3], [val loss 0.59451], [val acc 0.64067]\n",
      "*****************************************************\n",
      "[epoch 4], [iter 100 / 1124], [train loss 0.61451], [train acc 0.56221]\n",
      "[epoch 4], [iter 200 / 1124], [train loss 0.61032], [train acc 0.56542]\n",
      "[epoch 4], [iter 300 / 1124], [train loss 0.61433], [train acc 0.56307]\n",
      "[epoch 4], [iter 400 / 1124], [train loss 0.61562], [train acc 0.56962]\n",
      "[epoch 4], [iter 500 / 1124], [train loss 0.61985], [train acc 0.56100]\n",
      "[epoch 4], [iter 600 / 1124], [train loss 0.61803], [train acc 0.56584]\n",
      "[epoch 4], [iter 700 / 1124], [train loss 0.61847], [train acc 0.56023]\n",
      "[epoch 4], [iter 800 / 1124], [train loss 0.61550], [train acc 0.56756]\n",
      "[epoch 4], [iter 900 / 1124], [train loss 0.61957], [train acc 0.56321]\n",
      "[epoch 4], [iter 1000 / 1124], [train loss 0.61511], [train acc 0.56586]\n",
      "[epoch 4], [iter 1100 / 1124], [train loss 0.61650], [train acc 0.56197]\n",
      "------------------------------------------------------------\n",
      "[epoch 4], [val loss 0.56948], [val acc 0.66488]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 4], [val loss 0.56948], [val acc 0.66488]\n",
      "*****************************************************\n",
      "[epoch 5], [iter 100 / 1124], [train loss 0.58669], [train acc 0.58226]\n",
      "[epoch 5], [iter 200 / 1124], [train loss 0.58081], [train acc 0.58384]\n",
      "[epoch 5], [iter 300 / 1124], [train loss 0.58562], [train acc 0.58962]\n",
      "[epoch 5], [iter 400 / 1124], [train loss 0.58637], [train acc 0.58330]\n",
      "[epoch 5], [iter 500 / 1124], [train loss 0.58571], [train acc 0.58565]\n",
      "[epoch 5], [iter 600 / 1124], [train loss 0.58550], [train acc 0.58756]\n",
      "[epoch 5], [iter 700 / 1124], [train loss 0.58663], [train acc 0.58174]\n",
      "[epoch 5], [iter 800 / 1124], [train loss 0.58849], [train acc 0.58969]\n",
      "[epoch 5], [iter 900 / 1124], [train loss 0.58601], [train acc 0.58864]\n",
      "[epoch 5], [iter 1000 / 1124], [train loss 0.58659], [train acc 0.58844]\n",
      "[epoch 5], [iter 1100 / 1124], [train loss 0.58846], [train acc 0.58917]\n",
      "------------------------------------------------------------\n",
      "[epoch 5], [val loss 0.53224], [val acc 0.68714]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 5], [val loss 0.53224], [val acc 0.68714]\n",
      "*****************************************************\n",
      "[epoch 6], [iter 100 / 1124], [train loss 0.55694], [train acc 0.60062]\n",
      "[epoch 6], [iter 200 / 1124], [train loss 0.55654], [train acc 0.60115]\n",
      "[epoch 6], [iter 300 / 1124], [train loss 0.55985], [train acc 0.60100]\n",
      "[epoch 6], [iter 400 / 1124], [train loss 0.55571], [train acc 0.60565]\n",
      "[epoch 6], [iter 500 / 1124], [train loss 0.55286], [train acc 0.60500]\n",
      "[epoch 6], [iter 600 / 1124], [train loss 0.55511], [train acc 0.60586]\n",
      "[epoch 6], [iter 700 / 1124], [train loss 0.55610], [train acc 0.60585]\n",
      "[epoch 6], [iter 800 / 1124], [train loss 0.55659], [train acc 0.60844]\n",
      "[epoch 6], [iter 900 / 1124], [train loss 0.55566], [train acc 0.60540]\n",
      "[epoch 6], [iter 1000 / 1124], [train loss 0.55229], [train acc 0.60361]\n",
      "[epoch 6], [iter 1100 / 1124], [train loss 0.55977], [train acc 0.60562]\n",
      "------------------------------------------------------------\n",
      "[epoch 6], [val loss 0.50331], [val acc 0.70448]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 6], [val loss 0.50331], [val acc 0.70448]\n",
      "*****************************************************\n",
      "[epoch 7], [iter 100 / 1124], [train loss 0.52032], [train acc 0.62542]\n",
      "[epoch 7], [iter 200 / 1124], [train loss 0.52562], [train acc 0.62962]\n",
      "[epoch 7], [iter 300 / 1124], [train loss 0.52803], [train acc 0.62584]\n",
      "[epoch 7], [iter 400 / 1124], [train loss 0.52550], [train acc 0.62756]\n",
      "[epoch 7], [iter 500 / 1124], [train loss 0.52511], [train acc 0.62586]\n",
      "[epoch 7], [iter 600 / 1124], [train loss 0.52601], [train acc 0.62864]\n",
      "[epoch 7], [iter 700 / 1124], [train loss 0.52554], [train acc 0.62332]\n",
      "[epoch 7], [iter 800 / 1124], [train loss 0.52821], [train acc 0.62821]\n",
      "[epoch 7], [iter 900 / 1124], [train loss 0.52459], [train acc 0.62819]\n",
      "[epoch 7], [iter 1000 / 1124], [train loss 0.52331], [train acc 0.62117]\n",
      "[epoch 7], [iter 1100 / 1124], [train loss 0.52199], [train acc 0.62988]\n",
      "------------------------------------------------------------\n",
      "[epoch 7], [val loss 0.47912], [val acc 0.72698]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 7], [val loss 0.47912], [val acc 0.72698]\n",
      "*****************************************************\n",
      "[epoch 8], [iter 100 / 1124], [train loss 0.49157], [train acc 0.64417]\n",
      "[epoch 8], [iter 200 / 1124], [train loss 0.49124], [train acc 0.64130]\n",
      "[epoch 8], [iter 300 / 1124], [train loss 0.49847], [train acc 0.64023]\n",
      "[epoch 8], [iter 400 / 1124], [train loss 0.49663], [train acc 0.64174]\n",
      "[epoch 8], [iter 500 / 1124], [train loss 0.49610], [train acc 0.64585]\n",
      "[epoch 8], [iter 600 / 1124], [train loss 0.49554], [train acc 0.64332]\n",
      "[epoch 8], [iter 700 / 1124], [train loss 0.49555], [train acc 0.64096]\n",
      "[epoch 8], [iter 800 / 1124], [train loss 0.49153], [train acc 0.64776]\n",
      "[epoch 8], [iter 900 / 1124], [train loss 0.49546], [train acc 0.64796]\n",
      "[epoch 8], [iter 1000 / 1124], [train loss 0.49055], [train acc 0.64998]\n",
      "[epoch 8], [iter 1100 / 1124], [train loss 0.49189], [train acc 0.64567]\n",
      "------------------------------------------------------------\n",
      "[epoch 8], [val loss 0.44645], [val acc 0.74890]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 8], [val loss 0.44645], [val acc 0.74890]\n",
      "*****************************************************\n",
      "[epoch 9], [iter 100 / 1124], [train loss 0.46081], [train acc 0.66384]\n",
      "[epoch 9], [iter 200 / 1124], [train loss 0.46637], [train acc 0.66330]\n",
      "[epoch 9], [iter 300 / 1124], [train loss 0.46550], [train acc 0.66756]\n",
      "[epoch 9], [iter 400 / 1124], [train loss 0.46849], [train acc 0.66969]\n",
      "[epoch 9], [iter 500 / 1124], [train loss 0.46659], [train acc 0.66844]\n",
      "[epoch 9], [iter 600 / 1124], [train loss 0.46821], [train acc 0.66821]\n",
      "[epoch 9], [iter 700 / 1124], [train loss 0.46153], [train acc 0.66776]\n",
      "[epoch 9], [iter 800 / 1124], [train loss 0.46790], [train acc 0.66715]\n",
      "[epoch 9], [iter 900 / 1124], [train loss 0.46556], [train acc 0.66562]\n",
      "[epoch 9], [iter 1000 / 1124], [train loss 0.46858], [train acc 0.66943]\n",
      "[epoch 9], [iter 1100 / 1124], [train loss 0.46703], [train acc 0.66189]\n",
      "------------------------------------------------------------\n",
      "[epoch 9], [val loss 0.41118], [val acc 0.76673]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 9], [val loss 0.41118], [val acc 0.76673]\n",
      "*****************************************************\n",
      "[epoch 10], [iter 100 / 1124], [train loss 0.43433], [train acc 0.68307]\n",
      "[epoch 10], [iter 200 / 1124], [train loss 0.43803], [train acc 0.68584]\n",
      "[epoch 10], [iter 300 / 1124], [train loss 0.43957], [train acc 0.68321]\n",
      "[epoch 10], [iter 400 / 1124], [train loss 0.43601], [train acc 0.68864]\n",
      "[epoch 10], [iter 500 / 1124], [train loss 0.43566], [train acc 0.68540]\n",
      "[epoch 10], [iter 600 / 1124], [train loss 0.43459], [train acc 0.68819]\n",
      "[epoch 10], [iter 700 / 1124], [train loss 0.43546], [train acc 0.68796]\n",
      "[epoch 10], [iter 800 / 1124], [train loss 0.43556], [train acc 0.68562]\n",
      "[epoch 10], [iter 900 / 1124], [train loss 0.43663], [train acc 0.68964]\n",
      "[epoch 10], [iter 1000 / 1124], [train loss 0.43292], [train acc 0.68987]\n",
      "[epoch 10], [iter 1100 / 1124], [train loss 0.43317], [train acc 0.68087]\n",
      "------------------------------------------------------------\n",
      "[epoch 10], [val loss 0.38653], [val acc 0.78361]\n",
      "------------------------------------------------------------\n",
      "*****************************************************\n",
      "best record: [epoch 10], [val loss 0.38653], [val acc 0.78361]\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lancement de l'optimisation IHG\n",
    "ihg_optimization(train_loader, val_loader, model, criterion, optimizer, lambda_, epoch_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       precision  recall  f1-score   mae   tnr\n",
      "akiec       0.56    0.62      0.59  0.18  0.75\n",
      "bcc         0.72    0.78      0.75  0.14  0.81\n",
      "bkl         0.62    0.52      0.57  0.21  0.73\n",
      "df          0.57    0.66      0.61  0.17  0.77\n",
      "nv          0.88    0.86      0.87  0.11  0.85\n",
      "vasc        0.76    0.76      0.76  0.15  0.80\n",
      "mel         0.33    0.56      0.41  0.24  0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "def compute_metrics(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=None)\n",
    "    recall = recall_score(y_true, y_pred, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, average=None)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tnr = []\n",
    "    for i in range(len(cm)):\n",
    "        tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        tnr.append(tn / (tn + fp))\n",
    "    \n",
    "    labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc', 'mel']\n",
    "    print(f\"{'Class':<10}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}{'MAE':<10}{'TNR':<10}\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"{label:<10}{precision[i]:<10.2f}{recall[i]:<10.2f}{f1[i]:<10.2f}{mae:<10.2f}{tnr[i]:<10.2f}\")\n",
    "\n",
    "# Usage\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "compute_metrics(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.63\n",
      "recall: 0.68\n",
      "f1-score: 0.65\n",
      "mae: 0.17\n",
      "tnr: 0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "def compute_overall_metrics(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tn = np.sum(cm) - np.sum(np.diag(cm))\n",
    "    fp = np.sum(cm) - np.sum(cm, axis=0)\n",
    "    tnr = np.mean([tn[i] / (tn[i] + fp[i]) for i in range(len(fp))])\n",
    "\n",
    "    print(f\"precision: {precision:.2f}\")\n",
    "    print(f\"recall: {recall:.2f}\")\n",
    "    print(f\"f1-score: {f1:.2f}\")\n",
    "    print(f\"mae: {mae:.2f}\")\n",
    "    print(f\"tnr: {tnr:.2f}\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "compute_overall_metrics(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
