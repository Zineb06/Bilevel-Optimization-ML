{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_dataset2():\n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load the full MNIST training dataset\n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 20,000 samples were randomly selected\n",
    "    subset_indices = torch.randperm(len(full_dataset))[:20000]\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "    #  Divide 20,000 samples into 5,000 training sets, 5,000 validation sets, and 10,000 test sets\n",
    "    train_set, val_set, test_set = random_split(subset_dataset, [5000, 5000, 10000])\n",
    "\n",
    "    # Scramble the labeling of 2,500 samples in the training set\n",
    "    rand_indices = torch.randperm(len(train_set))[:2500]\n",
    "    for idx in rand_indices:\n",
    "        # A new tag is randomly generated\n",
    "        new_label = torch.randint(0, 10, (1,)).item()\n",
    "        train_set.dataset.dataset.targets[subset_indices[train_set.indices[idx]]] = new_label\n",
    "\n",
    "    # Create a data loader\n",
    "    trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "def test(net, testloader):\n",
    "    # Test the network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forecast\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f'Accuracy of the network on the 10000 test images: {accuracy} %')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(lower_lr, upper_lr):\n",
    "    # Define the neural network\n",
    "    net_old = SimpleNet().to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    lower_optimizer = optim.SGD(net_old.parameters(), lr=lower_lr, momentum=0.9)\n",
    "    upper_optimizer = optim.Adam(net_old.parameters(), lr=upper_lr)\n",
    "\n",
    "    trainloader, valloader, testloader = load_dataset2()\n",
    "\n",
    "    def lower_function(output, label, la):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) * la\n",
    "        return loss\n",
    "\n",
    "    def upper_function(output, label):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) + 0.01 * (torch.norm(net_old.fc.weight) + torch.norm(net_old.fc.bias))\n",
    "        return loss\n",
    "\n",
    "    def inner_loop(trainloader, net, la):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            lower_optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = lower_function(outputs, labels, la[i])\n",
    "            loss.backward()\n",
    "            lower_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        return running_loss\n",
    "\n",
    "    def outer_loop(trainloader, net, la):\n",
    "        upper_optimizer.zero_grad()\n",
    "\n",
    "        upper_loss = 0.0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = upper_function(outputs, labels)\n",
    "\n",
    "            upper_loss += loss\n",
    "\n",
    "        upper_loss.backward()\n",
    "        upper_optimizer.step()\n",
    "\n",
    "        return upper_loss\n",
    "\n",
    "    T = 10\n",
    "    la = torch.rand([5000, 1], requires_grad=True).to(device)\n",
    "    \n",
    "    for epoch in range(T):\n",
    "        lower_loss = inner_loop(trainloader, net_old, la)\n",
    "        \n",
    "        if epoch % 10 == 9:\n",
    "            print(f'[Epoch {epoch + 1}] lower_loss: {lower_loss / 200:.3f}')\n",
    "\n",
    "        upper_loss = outer_loop(trainloader, net_old, la)\n",
    "        \n",
    "    return net_old\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters for hyperparameter optimization\n",
    "    lower_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "    upper_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_lower_lr = 0.0\n",
    "    best_upper_lr = 0.0\n",
    "\n",
    "    for lower_lr in lower_lr_candidates:\n",
    "        for upper_lr in upper_lr_candidates:\n",
    "            print(f\"Training with lower_lr={lower_lr}, upper_lr={upper_lr}\")\n",
    "            net_trained = train(lower_lr, upper_lr)\n",
    "            _, _, testloader = load_dataset2()\n",
    "            accuracy = test(net_trained, testloader)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_lower_lr = lower_lr\n",
    "                best_upper_lr = upper_lr\n",
    "\n",
    "    print(f\"Best accuracy: {best_accuracy}%, Best lower_lr: {best_lower_lr}, Best upper_lr: {best_upper_lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f601c",
   "metadata": {},
   "source": [
    "## 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfa63f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n",
      "Training with lower_lr=0.01, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.387\n",
      "[Epoch 20] lower_loss: 0.376\n",
      "[Epoch 30] lower_loss: 0.407\n",
      "[Epoch 40] lower_loss: 0.361\n",
      "[Epoch 50] lower_loss: 0.361\n",
      "Accuracy of the network on the 10000 test images: 68.91 %\n",
      "Training with lower_lr=0.01, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.391\n",
      "[Epoch 20] lower_loss: 0.381\n",
      "[Epoch 30] lower_loss: 0.376\n",
      "[Epoch 40] lower_loss: 0.355\n",
      "[Epoch 50] lower_loss: 0.368\n",
      "Accuracy of the network on the 10000 test images: 65.0 %\n",
      "Training with lower_lr=0.01, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.391\n",
      "[Epoch 20] lower_loss: 0.374\n",
      "[Epoch 30] lower_loss: 0.357\n",
      "[Epoch 40] lower_loss: 0.352\n",
      "[Epoch 50] lower_loss: 0.345\n",
      "Accuracy of the network on the 10000 test images: 54.35 %\n",
      "Training with lower_lr=0.001, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.475\n",
      "[Epoch 20] lower_loss: 0.465\n",
      "[Epoch 30] lower_loss: 0.463\n",
      "[Epoch 40] lower_loss: 0.457\n",
      "[Epoch 50] lower_loss: 0.448\n",
      "Accuracy of the network on the 10000 test images: 59.91 %\n",
      "Training with lower_lr=0.001, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.368\n",
      "[Epoch 20] lower_loss: 0.360\n",
      "[Epoch 30] lower_loss: 0.357\n",
      "[Epoch 40] lower_loss: 0.353\n",
      "[Epoch 50] lower_loss: 0.351\n",
      "Accuracy of the network on the 10000 test images: 82.42 %\n",
      "Training with lower_lr=0.001, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.356\n",
      "[Epoch 20] lower_loss: 0.344\n",
      "[Epoch 30] lower_loss: 0.340\n",
      "[Epoch 40] lower_loss: 0.338\n",
      "[Epoch 50] lower_loss: 0.337\n",
      "Accuracy of the network on the 10000 test images: 81.81 %\n",
      "Training with lower_lr=0.0001, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.587\n",
      "[Epoch 20] lower_loss: 0.709\n",
      "[Epoch 30] lower_loss: 0.712\n",
      "[Epoch 40] lower_loss: 0.673\n",
      "[Epoch 50] lower_loss: 0.665\n",
      "Accuracy of the network on the 10000 test images: 89.21 %\n",
      "Training with lower_lr=0.0001, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.370\n",
      "[Epoch 20] lower_loss: 0.370\n",
      "[Epoch 30] lower_loss: 0.374\n",
      "[Epoch 40] lower_loss: 0.381\n",
      "[Epoch 50] lower_loss: 0.384\n",
      "Accuracy of the network on the 10000 test images: 84.71 %\n",
      "Training with lower_lr=0.0001, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.380\n",
      "[Epoch 20] lower_loss: 0.359\n",
      "[Epoch 30] lower_loss: 0.353\n",
      "[Epoch 40] lower_loss: 0.350\n",
      "[Epoch 50] lower_loss: 0.347\n",
      "Accuracy of the network on the 10000 test images: 82.07 %\n",
      "Best accuracy: 89.21%, Best lower_lr: 0.0001, Best upper_lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_dataset2():\n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load the full MNIST training dataset\n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 20,000 samples were randomly selected\n",
    "    subset_indices = torch.randperm(len(full_dataset))[:20000]\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "    #  Divide 20,000 samples into 5,000 training sets, 5,000 validation sets, and 10,000 test sets\n",
    "    train_set, val_set, test_set = random_split(subset_dataset, [5000, 5000, 10000])\n",
    "\n",
    "    # Scramble the labeling of 2,500 samples in the training set\n",
    "    rand_indices = torch.randperm(len(train_set))[:2500]\n",
    "    for idx in rand_indices:\n",
    "        # A new tag is randomly generated\n",
    "        new_label = torch.randint(0, 10, (1,)).item()\n",
    "        train_set.dataset.dataset.targets[subset_indices[train_set.indices[idx]]] = new_label\n",
    "\n",
    "    # Create a data loader\n",
    "    trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "def test(net, testloader):\n",
    "    # Test the network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forecast\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f'Accuracy of the network on the 10000 test images: {accuracy} %')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(lower_lr, upper_lr):\n",
    "    # Define the neural network\n",
    "    net_old = SimpleNet().to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    lower_optimizer = optim.SGD(net_old.parameters(), lr=lower_lr, momentum=0.9)\n",
    "    upper_optimizer = optim.Adam(net_old.parameters(), lr=upper_lr)\n",
    "\n",
    "    trainloader, valloader, testloader = load_dataset2()\n",
    "\n",
    "    def lower_function(output, label, la):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) * la\n",
    "        return loss\n",
    "\n",
    "    def upper_function(output, label):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) + 0.01 * (torch.norm(net_old.fc.weight) + torch.norm(net_old.fc.bias))\n",
    "        return loss\n",
    "\n",
    "    def inner_loop(trainloader, net, la):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            lower_optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = lower_function(outputs, labels, la[i])\n",
    "            loss.backward()\n",
    "            lower_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        return running_loss\n",
    "\n",
    "    def outer_loop(trainloader, net, la):\n",
    "        upper_optimizer.zero_grad()\n",
    "\n",
    "        upper_loss = 0.0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = upper_function(outputs, labels)\n",
    "\n",
    "            upper_loss += loss\n",
    "\n",
    "        upper_loss.backward()\n",
    "        upper_optimizer.step()\n",
    "\n",
    "        return upper_loss\n",
    "\n",
    "    T = 50\n",
    "    la = torch.rand([5000, 1], requires_grad=True).to(device)\n",
    "    \n",
    "    for epoch in range(T):\n",
    "        lower_loss = inner_loop(trainloader, net_old, la)\n",
    "        \n",
    "        if epoch % 10 == 9:\n",
    "            print(f'[Epoch {epoch + 1}] lower_loss: {lower_loss / 200:.3f}')\n",
    "\n",
    "        upper_loss = outer_loop(trainloader, net_old, la)\n",
    "        \n",
    "    return net_old\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters for hyperparameter optimization\n",
    "    lower_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "    upper_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_lower_lr = 0.0\n",
    "    best_upper_lr = 0.0\n",
    "\n",
    "    for lower_lr in lower_lr_candidates:\n",
    "        for upper_lr in upper_lr_candidates:\n",
    "            print(f\"Training with lower_lr={lower_lr}, upper_lr={upper_lr}\")\n",
    "            net_trained = train(lower_lr, upper_lr)\n",
    "            _, _, testloader = load_dataset2()\n",
    "            accuracy = test(net_trained, testloader)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_lower_lr = lower_lr\n",
    "                best_upper_lr = upper_lr\n",
    "\n",
    "    print(f\"Best accuracy: {best_accuracy}%, Best lower_lr: {best_lower_lr}, Best upper_lr: {best_upper_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc66873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
