{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63eded07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n",
      "Training with lower_lr=0.01, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.382\n",
      "Training with lower_lr=0.01, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.360\n",
      "Training with lower_lr=0.01, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.406\n",
      "Training with lower_lr=0.001, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.486\n",
      "Training with lower_lr=0.001, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.345\n",
      "Training with lower_lr=0.001, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.372\n",
      "Training with lower_lr=0.0001, upper_lr=0.01\n",
      "[Epoch 10] lower_loss: 0.592\n",
      "Training with lower_lr=0.0001, upper_lr=0.001\n",
      "[Epoch 10] lower_loss: 0.386\n",
      "Training with lower_lr=0.0001, upper_lr=0.0001\n",
      "[Epoch 10] lower_loss: 0.398\n",
      "Best accuracy: 83.08%, Best lower_lr: 0.0001, Best upper_lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_dataset2():\n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load the full MNIST training dataset\n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 20,000 samples were randomly selected\n",
    "    subset_indices = torch.randperm(len(full_dataset))[:20000]\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "    #  Divide 20,000 samples into 5,000 training sets, 5,000 validation sets, and 10,000 test sets\n",
    "    train_set, val_set, test_set = random_split(subset_dataset, [5000, 5000, 10000])\n",
    "\n",
    "    # Scramble the labeling of 2,500 samples in the training set\n",
    "    rand_indices = torch.randperm(len(train_set))[:2500]\n",
    "    for idx in rand_indices:\n",
    "        # A new tag is randomly generated\n",
    "        new_label = torch.randint(0, 10, (1,)).item()\n",
    "        train_set.dataset.dataset.targets[subset_indices[train_set.indices[idx]]] = new_label\n",
    "\n",
    "    # Create a data loader\n",
    "    trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "def test(net, testloader):\n",
    "    # Test the network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forecast\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(lower_lr, upper_lr):\n",
    "    # Define the neural network\n",
    "    net_old = SimpleNet().to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    lower_optimizer = optim.SGD(net_old.parameters(), lr=lower_lr, momentum=0.9)\n",
    "    upper_optimizer = optim.Adam(net_old.parameters(), lr=upper_lr)\n",
    "\n",
    "    trainloader, valloader, testloader = load_dataset2()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    def lower_function(output, label, la):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) * la\n",
    "        return loss\n",
    "\n",
    "    def upper_function(output, label):\n",
    "        crossentropy = nn.CrossEntropyLoss()\n",
    "        loss = crossentropy(output, label) + 0.01 * (torch.norm(net_old.fc.weight) + torch.norm(net_old.fc.bias))\n",
    "        return loss\n",
    "\n",
    "    def inner_loop(trainloader, net, la):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            lower_optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = lower_function(outputs, labels, la[i])\n",
    "            loss.backward()\n",
    "            lower_optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        return running_loss\n",
    "\n",
    "    def outer_loop(trainloader, net, la):\n",
    "        upper_optimizer.zero_grad()\n",
    "\n",
    "        upper_loss = 0.0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = upper_function(outputs, labels)\n",
    "\n",
    "            upper_loss += loss\n",
    "\n",
    "        upper_loss.backward()\n",
    "        upper_optimizer.step()\n",
    "\n",
    "        return upper_loss\n",
    "\n",
    "    T = 10\n",
    "    la = torch.rand([5000, 1], requires_grad=True).to(device)\n",
    "    \n",
    "    for epoch in range(T):\n",
    "        lower_loss = inner_loop(trainloader, net_old, la)\n",
    "        train_losses.append(lower_loss)\n",
    "\n",
    "        if epoch % 10 == 9:\n",
    "            print(f'[Epoch {epoch + 1}] lower_loss: {lower_loss / 200:.3f}')\n",
    "\n",
    "        upper_loss = outer_loop(trainloader, net_old, la)\n",
    "        val_losses.append(upper_loss)\n",
    "\n",
    "        train_accuracy = test(net_old, trainloader)\n",
    "        val_accuracy = test(net_old, valloader)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "    return net_old, train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters for hyperparameter optimization\n",
    "    lower_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "    upper_lr_candidates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_lower_lr = 0.0\n",
    "    best_upper_lr = 0.0\n",
    "\n",
    "    for lower_lr in lower_lr_candidates:\n",
    "        for upper_lr in upper_lr_candidates:\n",
    "            print(f\"Training with lower_lr={lower_lr}, upper_lr={upper_lr}\")\n",
    "            net_trained, train_losses, val_losses, train_accuracies, val_accuracies = train(lower_lr, upper_lr)\n",
    "            _, _, testloader = load_dataset2()\n",
    "            accuracy = test(net_trained, testloader)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_lower_lr = lower_lr\n",
    "                best_upper_lr = upper_lr\n",
    "\n",
    "    print(f\"Best accuracy: {best_accuracy}%, Best lower_lr: {best_lower_lr}, Best upper_lr: {best_upper_lr}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161825d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1371478360.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    epochs = range(1, 21)  # Assuming 20 epochs\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "    epochs = range(1, 21)  # Assuming 20 epochs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d5e6b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1744056592.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.subplot(2, 2, 2)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Plotting\n",
    "\n",
    "    # Plot Training and Validation Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891c3281",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1305686293.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.subplot(2, 2, 3)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " # Plot Best Hyperparameters\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(best_lower_lr, best_upper_lr, color='red', label='Best Hyperparameters')\n",
    "    plt.xlabel('Lower Learning Rate')\n",
    "    plt.ylabel('Upper Learning Rate')\n",
    "    plt.title('Best Hyperparameters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7451239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
