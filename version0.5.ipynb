{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a20a3c7",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Cz1544252489/DailyWork/blob/main/jupyter%20notebook/version0.5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7748856-312f-4b93-aa05-13f72cba6768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7748856-312f-4b93-aa05-13f72cba6768",
    "outputId": "cf290f11-2b0e-4f06-bd5f-7e560ebe1ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:03<00:00, 2561193.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 3189349.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 5137683.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_dataset2():\n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load the full MNIST training dataset\n",
    "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "    # 20,000 samples were randomly selected\n",
    "    subset_indices = torch.randperm(len(full_dataset))[:20000]\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "    #  Divide 20,000 samples into 5,000 training sets, 5,000 validation sets, and 10,000 test sets\n",
    "    train_set, val_set, test_set = random_split(subset_dataset, [5000, 5000, 10000])\n",
    "\n",
    "    # Scramble the labeling of 2,500 samples in the training set\n",
    "    rand_indices = torch.randperm(len(train_set))[:2500]\n",
    "    for idx in rand_indices:\n",
    "        # A new tag is randomly generated\n",
    "        new_label = torch.randint(0, 10, (1,)).item()\n",
    "        train_set.dataset.dataset.targets[subset_indices[train_set.indices[idx]]] = new_label\n",
    "\n",
    "    # Create a data loader\n",
    "    trainloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test_set, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "def test(net, testloader):\n",
    "    # Test the network\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # forecast\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n",
    "\n",
    "\n",
    "trainloader, valloader, testloader = load_dataset2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1199e6b0-07f1-42bd-a2a6-3f787c1435ca",
   "metadata": {
    "id": "1199e6b0-07f1-42bd-a2a6-3f787c1435ca"
   },
   "outputs": [],
   "source": [
    "# Define neural networks\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net_old = SimpleNet()\n",
    "\n",
    "N = 5000\n",
    "la = torch.rand([N,1],requires_grad=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f882bed-8e75-4ec4-87ba-176e49c9ed8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f882bed-8e75-4ec4-87ba-176e49c9ed8c",
    "outputId": "d7bbd9ca-e1b9-458b-99e0-a45b9ad08418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 11.0 %\n",
      "[Epoch 10] lower_loss: 0.548\n",
      "[Epoch 20] lower_loss: 0.497\n",
      "[Epoch 30] lower_loss: 0.554\n",
      "[Epoch 40] lower_loss: 0.415\n",
      "[Epoch 50] lower_loss: 0.460\n",
      "[Epoch 60] lower_loss: 0.428\n",
      "[Epoch 70] lower_loss: 0.417\n",
      "[Epoch 80] lower_loss: 0.580\n",
      "[Epoch 90] lower_loss: 0.521\n",
      "[Epoch 100] lower_loss: 0.525\n",
      "upper_loss: 0.713\n",
      "Accuracy of the network on the 10000 test images: 51.31 %\n"
     ]
    }
   ],
   "source": [
    "# Use the same parameters to compare how well and badly optimizations are good\n",
    "net = copy.deepcopy(net_old).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "def lower_function(output, label, la):\n",
    "    crossentropy = nn.CrossEntropyLoss()\n",
    "    loss = crossentropy(output, label)*la\n",
    "    return loss\n",
    "\n",
    "def upper_function(output, label):\n",
    "    crossentropy = nn.CrossEntropyLoss()\n",
    "    loss = crossentropy(output, label)+0.01*(torch.norm(net.fc.weight)+torch.norm(net.fc.bias))\n",
    "    return loss\n",
    "\n",
    "# SGD is significantly better than Adam's\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# Define the inner layer loop\n",
    "def inner_loop(trainloader, net, la):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = lower_function(outputs, labels, la[i])\n",
    "        #s = torch.cat((net.fc.weight.data, net.fc.bias.data.view(-1,1)), dim=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss, net\n",
    "\n",
    "# Test the network for the first time\n",
    "test(net, testloader)\n",
    "\n",
    "\n",
    "T = 100\n",
    "# Train the network\n",
    "for epoch in range(T):\n",
    "    lower_loss, net  = inner_loop(trainloader, net, la)\n",
    "\n",
    "    s = torch.cat((net.fc.weight.data, net.fc.bias.data.view(-1,1)), dim=1).view(-1)\n",
    "    s_grad = torch.cat((net.fc.weight.grad.data, net.fc.bias.grad.data.view(-1,1)), dim=1).view(-1)\n",
    "\n",
    "    if epoch % 10 ==9:\n",
    "        print(f'[Epoch {epoch + 1}] lower_loss: {lower_loss / 200:.3f}')\n",
    "\n",
    "    B = la.grad\n",
    "    A = torch.cat((net.fc.weight.grad.data, net.fc.bias.grad.data.view(-1,1)), dim=1).view(-1)\n",
    "\n",
    "\n",
    "    upper_loss = 0.0\n",
    "    for i, data in enumerate(valloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = upper_function(outputs, labels)\n",
    "\n",
    "        upper_loss += loss\n",
    "\n",
    "\n",
    "print(f'upper_loss: {upper_loss / 200:.3f}')\n",
    "\n",
    "\n",
    "test(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2tJb7sJbq03L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tJb7sJbq03L",
    "outputId": "7dadf27c-22f4-4545-c665-461d73b7c49c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 51.31 %\n"
     ]
    }
   ],
   "source": [
    "test(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a3f59d7-678c-40c0-80e8-50123f4b2a9b",
   "metadata": {
    "id": "2a3f59d7-678c-40c0-80e8-50123f4b2a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(net.fc.weight.grad.shape,net.fc.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f7457c2-8808-45ff-a5a3-ebde14c72219",
   "metadata": {
    "id": "3f7457c2-8808-45ff-a5a3-ebde14c72219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7850])\n",
      "torch.Size([7850])\n"
     ]
    }
   ],
   "source": [
    "B = la.grad\n",
    "A = net.fc.weight.grad\n",
    "B = net.fc.bias.grad.view(-1,1)\n",
    "C = torch.cat((A, B),dim=1)\n",
    "s = torch.cat((net.fc.weight.data, net.fc.bias.data.view(-1,1)), dim=1).view(-1)\n",
    "s_grad = torch.cat((net.fc.weight.grad.data, net.fc.bias.grad.data.view(-1,1)), dim=1).view(-1)\n",
    "print(s.shape)\n",
    "print(s_grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06422cd-d925-4055-84e1-dfbf839da975",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a06422cd-d925-4055-84e1-dfbf839da975",
    "outputId": "f4c996b6-7b12-416e-9f25-17b38f92a441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(inputs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d550bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
